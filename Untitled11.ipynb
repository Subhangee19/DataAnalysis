{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMg4b/CqCNDLvrmDd81fDi8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Subhangee19/DataAnalysis/blob/main/Untitled11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "## Problem Statement\n",
        "We aim to predict sentiment (positive/negative) from Steam game reviews to support market analysis and content recommendation.\n",
        "\n",
        "## Business Importance\n",
        "Understanding review sentiment helps game publishers, platforms, and developers improve game design and target marketing campaigns more effectively.\n",
        "\n",
        "## Objective\n",
        "Build an end-to-end NLP pipeline using:\n",
        "- TF-IDF + Logistic Regression\n",
        "- Word2Vec + Logistic Regression\n",
        "- LSTM\n",
        "And compare them to determine the most effective model.\n"
      ],
      "metadata": {
        "id": "wM1VFjAkBBlw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "FoXlDl_etw-R"
      },
      "outputs": [],
      "source": [
        "!pip install nltk gensim transformers datasets scikit-learn bs4\n",
        "!pip install kagglehub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                             f1_score, confusion_matrix, classification_report, roc_auc_score, RocCurveDisplay)\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from bs4 import BeautifulSoup\n"
      ],
      "metadata": {
        "id": "Xwg60TyTY_rQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "b7NTWsyeZGrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import kagglehub\n",
        "\n",
        "# Download and get path\n",
        "path = kagglehub.dataset_download(\"piyushagni5/sentiment-analysis-for-steam-reviews\")\n",
        "print(\"Path to dataset files:\", path)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mm6gm4vvZJzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data Acquisition\n",
        "\n",
        "We use the Kaggle dataset: **Sentiment Analysis for Steam Reviews**  \n",
        "URL: https://www.kaggle.com/datasets/piyushagni5/sentiment-analysis-for-steam-reviews\n",
        "\n",
        "The dataset contains:\n",
        "- `user_review`: Text review.\n",
        "- `user_suggestion`: 1 = positive, 0 = negative.\n"
      ],
      "metadata": {
        "id": "3nL8WxycBMCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(f\"{path}/train.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "6ft7By_XbOUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()\n",
        "df.describe()\n",
        "df.isnull().sum()\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "qU7j760qlc9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Exploratory Data Analysis\n",
        "\n",
        "In this section, we explore:\n",
        "- The balance between positive and negative reviews.\n",
        "- The average length of reviews.\n",
        "- Frequent words using word clouds.\n"
      ],
      "metadata": {
        "id": "TAZ6Eg5UCPuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Sentiment label distribution\n",
        "sns.countplot(x='user_suggestion', data=df, palette='Set2')\n",
        "plt.title('Distribution of Sentiment Labels')\n",
        "plt.xlabel('Sentiment (1=Positive, 0=Negative)')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V_Sf4pXUB1Ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a column for review length\n",
        "df['review_length'] = df['user_review'].astype(str).apply(len)\n",
        "\n",
        "# Plot the length distribution\n",
        "sns.histplot(df['review_length'], bins=50, color='skyblue')\n",
        "plt.title('Distribution of Review Lengths')\n",
        "plt.xlabel('Number of Characters in Review')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "auscipX0B8gZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WordClouds\n",
        "text_pos = ' '.join(df[df['user_suggestion'] == 1]['user_review'])\n",
        "text_neg = ' '.join(df[df['user_suggestion'] == 0]['user_review'])\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(WordCloud(width=500, height=300, background_color='white').generate(text_pos))\n",
        "plt.title('Positive Reviews')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(WordCloud(width=500, height=300, background_color='white').generate(text_neg))\n",
        "plt.title('Negative Reviews')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lM_oF0Ihls2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "\n",
        "df['cleaned_review'] = df['user_review'].astype(str).apply(clean_text)\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "Obc3V0D0lyOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['sentiment'] = df['user_suggestion'].map({1: 'positive', 0: 'negative'})\n",
        "df['sentiment_label'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n"
      ],
      "metadata": {
        "id": "usTVm4cal0xO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"sentiment\"].value_counts()"
      ],
      "metadata": {
        "id": "C668pOwfyYN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df['cleaned_review']\n",
        "y = df['sentiment_label']\n",
        "\n",
        "# Single unified split for all models\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "DjZVplXQuOfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# user_suggestion: 1 = positive, 0 = negative\n",
        "df['sentiment'] = df['user_suggestion'].map({1: 'positive', 0: 'negative'})\n",
        "df['sentiment_label'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
        "df[['cleaned_review', 'sentiment', 'sentiment_label']].head()\n"
      ],
      "metadata": {
        "id": "dIakx8fWcZvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# Class distribution\n",
        "sns.countplot(data=df, x='sentiment')\n",
        "plt.title('Sentiment Class Distribution')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "# Show class balance numerically too\n",
        "df['sentiment'].value_counts()\n"
      ],
      "metadata": {
        "id": "fBqywOMjuOs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=3000, stop_words='english')\n",
        "X_tfidf = vectorizer.fit_transform(df['cleaned_review'])\n",
        "y = df['sentiment_label']\n"
      ],
      "metadata": {
        "id": "_A-v6f-NcdaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Text Preprocessing\n"
      ],
      "metadata": {
        "id": "3PiT7XSxCaHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['tokens'] = df['cleaned_review'].apply(lambda x: x.split())\n",
        "\n"
      ],
      "metadata": {
        "id": "vH7MPAZoCXIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word2Vec Embedding\n",
        "We train Word2Vec to create dense vector representations of words and average them to get review-level features.\n"
      ],
      "metadata": {
        "id": "DL5N03SkDQJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "df['tokens'] = df['cleaned_review'].apply(lambda x: x.split())\n",
        "\n",
        "# Train Word2Vec\n",
        "w2v_model = Word2Vec(sentences=df['tokens'], vector_size=100, window=5, min_count=2, workers=4, epochs=30)\n",
        "\n",
        "# Function to get average embedding\n",
        "def get_avg_vector(tokens):\n",
        "    valid_tokens = [word for word in tokens if word in w2v_model.wv]\n",
        "    return np.mean(w2v_model.wv[valid_tokens], axis=0) if valid_tokens else np.zeros(100)\n",
        "\n",
        "# Apply\n",
        "X_w2v = np.vstack(df['tokens'].apply(get_avg_vector))\n"
      ],
      "metadata": {
        "id": "GuDtv8o8ekkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Feature Engineering\n",
        "\n",
        "We apply two different feature extraction methods:\n",
        "- **TF-IDF Vectorization**: Converts documents into a matrix of term frequency and inverse document frequency.\n",
        "- **Word2Vec Embeddings**: Learns dense vector representations of words from context.\n",
        "\n",
        "These will be used as input features for our models.\n"
      ],
      "metadata": {
        "id": "jSBTTpMpDIWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(max_features=3000, stop_words='english')\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "X_train_w2v = np.vstack(X_train.apply(get_avg_vector))\n",
        "X_test_w2v = np.vstack(X_test.apply(get_avg_vector))\n"
      ],
      "metadata": {
        "id": "pVR0SWfbchFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_tfidf = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "model_tfidf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "y_pred_tfidf = model_tfidf.predict(X_test_tfidf)\n",
        "\n",
        "print(\"TF-IDF + Logistic Regression\")\n",
        "print(classification_report(y_test, y_pred_tfidf))\n"
      ],
      "metadata": {
        "id": "0VWTvKtickZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm_tfidf = confusion_matrix(y_test, y_pred_tfidf)\n",
        "sns.heatmap(cm_tfidf, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('TF-IDF + Logistic Regression')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "niw9qnu-mbZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RocCurveDisplay.from_estimator(model_tfidf, X_test_tfidf, y_test)\n",
        "plt.title('ROC Curve - TF-IDF Model')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uY7Uf0STmfFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model_w2v = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "model_w2v.fit(X_train_w2v, y_train)\n",
        "\n",
        "y_pred_w2v = model_w2v.predict(X_test_w2v)\n"
      ],
      "metadata": {
        "id": "-JgZ4aLemiED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm_w2v = confusion_matrix(y_test, y_pred_w2v)\n",
        "\n",
        "sns.heatmap(cm_w2v, annot=True, fmt='d', cmap='Purples')\n",
        "plt.title('Word2Vec + Logistic Regression')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QasnsCxxmj5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RocCurveDisplay.from_estimator(model_w2v, X_test_w2v, y_test)\n",
        "plt.title('ROC Curve - Word2Vec Model')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "U3HjKnMbmlb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "# Test Example\n",
        "result = classifier(\"This game is absolutely terrible.\")\n",
        "print(result)\n",
        "\n",
        "# Apply on full dataset (optional, computationally heavy)\n",
        "# df['bert_sentiment'] = df['user_review'].apply(lambda x: classifier(x)[0]['label'])\n"
      ],
      "metadata": {
        "id": "qhV80m_BmrHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Deep Learning Model – LSTM\n",
        "\n",
        "While traditional models like TF-IDF + Logistic Regression and Word2Vec + Logistic Regression are effective for capturing frequency-based or semantic features, they cannot fully capture sequential relationships in text.\n",
        "\n",
        "LSTM (Long Short-Term Memory) is a type of recurrent neural network (RNN) that captures long-term dependencies in sequences, making it highly suitable for text data where word order matters.\n",
        "\n",
        "In this section, we implement an LSTM-based model to classify sentiment from Steam reviews.\n"
      ],
      "metadata": {
        "id": "eGDMT0Fy_9kQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n"
      ],
      "metadata": {
        "id": "0s236aTR__h6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "vocab_size = 10000\n",
        "max_length = 100\n",
        "embedding_dim = 100\n",
        "trunc_type = 'post'\n",
        "padding_type = 'post'\n",
        "oov_tok = \"<OOV>\"\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# Convert text to sequences\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=100, padding='post', truncating='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=100, padding='post', truncating='post')\n",
        "\n"
      ],
      "metadata": {
        "id": "uHNBuLe6AGNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    Bidirectional(LSTM(64, return_sequences=True)),\n",
        "    Dropout(0.3),\n",
        "    Bidirectional(LSTM(32)),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model_lstm.summary()\n"
      ],
      "metadata": {
        "id": "ATR2V3DXAOSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "history = model_lstm.fit(\n",
        "    X_train_pad, y_train,\n",
        "    epochs=10,\n",
        "    validation_data=(X_test_pad, y_test),\n",
        "    batch_size=64,\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "DVNEmKAtATNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate\n",
        "loss, accuracy = model_lstm.evaluate(X_test_pad, y_test)\n",
        "print(f'✅ Test Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "id": "JC6kCtP8AibM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Predictions\n",
        "y_pred_probs = model_lstm.predict(X_test_pad)\n",
        "y_pred = (y_pred_probs >= 0.5).astype(int)\n",
        "y_pred_lstm = y_pred\n",
        "\n",
        "# Classification Report\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens')\n",
        "plt.title('LSTM Model Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6h_ZxXFqAlb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comparison_df = pd.DataFrame({\n",
        "    'Model': [\n",
        "        'TF-IDF + Logistic Regression',\n",
        "        'Word2Vec + Random Forest',\n",
        "        'LSTM (Deep Learning)'\n",
        "    ],\n",
        "    'Accuracy': [\n",
        "        accuracy_score(y_test, y_pred_tfidf),\n",
        "        accuracy_score(y_test, y_pred_w2v),\n",
        "        accuracy_score(y_test, y_pred_lstm)\n",
        "    ],\n",
        "    'Precision': [\n",
        "        precision_score(y_test, y_pred_tfidf),\n",
        "        precision_score(y_test, y_pred_w2v),\n",
        "        precision_score(y_test, y_pred_lstm)\n",
        "    ],\n",
        "    'Recall': [\n",
        "        recall_score(y_test, y_pred_tfidf),\n",
        "        recall_score(y_test, y_pred_w2v),\n",
        "        recall_score(y_test, y_pred_lstm)\n",
        "    ],\n",
        "    'F1 Score': [\n",
        "        f1_score(y_test, y_pred_tfidf),\n",
        "        f1_score(y_test, y_pred_w2v),\n",
        "        f1_score(y_test, y_pred_lstm)\n",
        "    ]\n",
        "})\n",
        "print(comparison_df.sort_values(by='Accuracy', ascending=False))\n"
      ],
      "metadata": {
        "id": "S2-43KewvuLw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}